---
title: "Crime Trend and Spatial Analysis in Chicago (2010 - 2022)"
author: Satya
date: Oct 22, 2024
date-format: long
editor: 
  markdown: 
    wrap: 72
---

# Introduction

Understanding crime trends and patterns is crucial for developing
effective crime prevention and resource allocation strategies. By
visualizing year-wise distributions, we gain insight into:

1.Temporal changes in crime rates.

2.Potential correlations with social, economic, or policy changes.

3.Identifying years of significant increase or decrease in criminal
activities, helping stakeholders pinpoint impactful interventions.

# Welcome to the Crime Analysis Project

This website presents a detailed analysis of crime trends and spatial
distributions in Chicago from 2010 to 2022.

# Materials and methods implemented

Data Cleaning and Preparation I am using the dplyr and tidyr packages in
R Studio to clean and organize the dataset, removing any inconsistencies
or missing values to ensure data accuracy.

Time-Series Analysis I am leveraging ggplot2 to plot trends in crime
rates over time, analyzing how crime levels are evolving throughout the
study period.

Spatial Analysis Using the sf package, I am mapping crime locations to
visualize their spatial distribution across Chicago's neighborhoods,
helping to identify patterns and areas of concern.

Cluster Analysis I am applying clustering techniques to identify crime
hotspots and determine whether certain crime types are showing spatial
concentrations in specific regions of the city.

Predictive Modeling I am using machine learning techniques, such as
caret and randomForest, to build models that predict crime occurrences
based on time and location, providing insights for crime prevention
strategies.

Required packages:

# Load necessary libraries

```{r message=FALSE}
library(dplyr)
library(tidyr)
```

# Load the dataset

```{r message=FALSE}
df <- read.csv("data/data.csv", stringsAsFactors = FALSE)
```

# View the first few rows of the dataset

```{r}
head(df)
```

# DATA CLEANING AND PREPERATION

# Dropping duplicate rows

```{r  message=FALSE}
df <- df %>% distinct()
```

# Removing rows with any missing values

```{r message=FALSE}
df <- df %>% drop_na()
```

# Identifying and cleaning inconsistent values (example: convert character columns to lowercase)

```{r message=FALSE}
df <- df %>% mutate_if(is.character, tolower)
```

# Replacing any incorrect or placeholder values like "NA" or "unknown" with NA

```{r message=FALSE}
df <- df %>%
  mutate(across(where(is.character), ~ na_if(., "NA"))) %>%
  mutate(across(where(is.character), ~na_if(., "unknown")))
```

# Re-checking for missing values and inconsistencies in the data

```{r message=FALSE}
summary(df)
```

# Barplot

The below bar graph, Distribution of Crimes by Year, provides a key
visualization from the project titled "Crime Trend and Spatial Analysis
in Chicago (2010–2022)." This project explores the trends, spatial
distribution, and predictive factors of crimes in Chicago over 13 years.

```{r message=FALSE}
library(ggplot2)

# Create a histogram of the Year column
ggplot(df, aes(x = Year)) +
  geom_histogram(binwidth = 1, fill = "beige", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Crimes by Year", x = "Year", y = "Frequency") +
  theme_minimal()

```

# TIME SERIES ANALYSIS

```{r message=FALSE}

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Convert 'Date' column to Date type and extract month-year for aggregation
df$Date <- as.Date(df$Date, format="%m/%d/%Y %I:%M:%S %p")
df$Month <- format(df$Date, "%Y-%m")

# Aggregate crime counts by month
monthly_crime_counts <- df %>%
  group_by(Month) %>%
  summarise(Crime_Count = n())

monthly_crime_counts$Month <- as.Date(paste0(monthly_crime_counts$Month, "-01"))
monthly_crime_counts <- monthly_crime_counts[format(monthly_crime_counts$Month, "%Y") != "2023", ]


# Plot the time-series data
ggplot(monthly_crime_counts, aes(x = as.Date(Month), y = Crime_Count)) +
  geom_line(color = "blue") +
  labs(title = "Monthly Crime Counts Over Time",
       x = "Date",
       y = "Crime Count") +
  theme_minimal()

```

```{r message=FALSE}

# Extract Year and Month separately
df$Year <- format(df$Date, "%Y")
df$Month <- format(df$Date, "%m")

# Aggregate data by Year and Month to calculate monthly crime rates per year
monthly_crime_rate <- df %>%
  group_by(Year, Month) %>%
  summarise(Crime_Count = n()) %>%
  ungroup()

# Convert Month to a factor to ensure correct ordering on the x-axis
monthly_crime_rate$Month <- factor(monthly_crime_rate$Month, levels = sprintf("%02d", 1:12), labels = month.abb)

monthly_crime_rate <- monthly_crime_rate[monthly_crime_rate$Year != "2023", ]



# Plot multiple lines for each year
ggplot(monthly_crime_rate, aes(x = Month, y = Crime_Count, color = Year, group = Year)) +
  geom_line(size = 1) +
  labs(title = "Monthly Crime Rates by Year",
       x = "Month",
       y = "Crime Count") +
  theme_minimal() +
  theme(legend.position = "right")


```

# SPATIAL ANALYSIS

```{r}
head(df)
```

```{r message=FALSE}

# Load necessary libraries
library(sf)
library(ggplot2)
library(ggmap)

library(ggmap)
register_stadiamaps(key = "646b60c3-8bef-49e4-bc52-805e18cdae42")


# Convert the data to an sf object with crime location coordinates
crime_data_sf <- st_as_sf(df, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant")

# Get a basemap of Chicago using ggmap
# Ensure you have the ggmap API key for Google Maps if you choose source = "google"
chicago_map <- get_stadiamap(
  bbox = c(left = -87.9401, bottom = 41.6445, right = -87.5237, top = 42.0230), 
  zoom = 11, 
  maptype = "stamen_terrain"
)

```

```{r, fig.width=30, fig.height=28}

# Plot crime locations on the map of Chicago

ggmap(chicago_map) +
  geom_sf(data = crime_data_sf, inherit.aes = FALSE, color = "red", size = 0.5, alpha = 0.7) +
  labs(title = "Crime Distribution Across Chicago") +
  theme_minimal()


```

# CLUSTER ANALYSIS

```{r message=FALSE}

# Load necessary libraries
library(ggplot2)
library(dplyr)
library(sf)

remove_outliers <- function(df, col1, col2) {
  # Calculate IQR for col1 (Latitude) and col2 (Longitude)
  Q1_col1 <- quantile(df[[col1]], 0.25)
  Q3_col1 <- quantile(df[[col1]], 0.75)
  IQR_col1 <- Q3_col1 - Q1_col1
  
  Q1_col2 <- quantile(df[[col2]], 0.25)
  Q3_col2 <- quantile(df[[col2]], 0.75)
  IQR_col2 <- Q3_col2 - Q1_col2
  
  # Define lower and upper bounds for outliers
  lower_bound_col1 <- Q1_col1 - 1.5 * IQR_col1
  upper_bound_col1 <- Q3_col1 + 1.5 * IQR_col1
  
  lower_bound_col2 <- Q1_col2 - 1.5 * IQR_col2
  upper_bound_col2 <- Q3_col2 + 1.5 * IQR_col2
  
  # Remove rows where either Latitude or Longitude is an outlier
  df_cleaned <- df[df[[col1]] >= lower_bound_col1 & df[[col1]] <= upper_bound_col1, ]
  df_cleaned <- df_cleaned[df_cleaned[[col2]] >= lower_bound_col2 & df_cleaned[[col2]] <= upper_bound_col2, ]
  
  return(df_cleaned)
}


# Remove outliers from both Latitude and Longitude columns
df <- remove_outliers(df, "Latitude", "Longitude")

# Ensure Latitude and Longitude are numeric
df$Latitude <- as.numeric(df$Latitude)
df$Longitude <- as.numeric(df$Longitude)

# Create a data frame with only the relevant columns (Description, Latitude, Longitude)
map_data <- df %>% select(Description, Latitude, Longitude)

# Remove rows with missing coordinates or descriptions
map_data <- map_data %>% filter(!is.na(Latitude) & !is.na(Longitude))

# Convert to an sf (spatial) object for mapping
map_sf <- st_as_sf(map_data, coords = c("Longitude", "Latitude"), crs = 4326)

# Plot the map with ggplot
ggplot(map_sf) +
  geom_sf(aes(color = Description), size = 1) +
  scale_color_manual(values = rainbow(length(unique(map_data$Description)))) +  # Use rainbow colors for uniqueness
  theme_minimal() +
  labs(title = "Crime Descriptions by Location",
       subtitle = "Map of Crime Descriptions in Chicago",
       color = "Description") +
  theme(legend.position = "right") +
  theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())


```

# Prediction Models

```{r message=FALSE}
library(dplyr)
library(tidyr)
library(caret)

# Assuming your data is stored in a data frame named 'df'

# Step 1: Data Preprocessing
# Convert 'Arrest' column to numeric (if it's not already in numeric format)

# Select relevant columns and handle missing data
df_clean <- df %>%
  select(Primary.Type, Description, Community.Area, Arrest)

df_clean$Arrest <- as.numeric(df_clean$Arrest == "true")


df_encoded <- df_clean[complete.cases(df_clean), ]


# Step 2: One-Hot Encoding
df_encoded <- df_clean %>%
  mutate(across(c(Primary.Type, Description, Community.Area), as.factor)) %>%
  mutate(across(c(Primary.Type, Description, Community.Area), ~as.numeric(factor(.)))) %>%
  na.omit()

# Step 3: Split the data into training and testing sets
set.seed(123)  # Set a seed for reproducibility
shuffled_index <- sample(1:nrow(df_encoded))

# Split the data (80% for training, 20% for testing)
train_index <- shuffled_index[1:floor(0.8 * length(shuffled_index))]
test_index <- shuffled_index[(floor(0.8 * length(shuffled_index)) + 1):length(shuffled_index)]


train_data <- df_encoded[train_index, ]
test_data <- df_encoded[-train_index, ]


# Step 4: Train a logistic regression model
model <- glm(train_data$Arrest ~ ., data = train_data, family = "binomial")

# Step 5: Model Summary
summary(model)

# Step 6: Predict on the test data
predictions <- predict(model, newdata = test_data, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)

# Step 7: Evaluate the model
confusion_matrix <- confusionMatrix(factor(predicted_class), factor(test_data$Arrest))
confusion_matrix

# Step 8: Calculate AUC (Area Under the Curve)
library(pROC)
roc_curve <- roc(test_data$Arrest, predictions)
auc(roc_curve)


```

# **Results**

The prediction model used in this project was Logistic Regression,
designed to predict whether an arrest would occur based on crime-related
features such as Primary Type, Description, and Community Area. Here are
the key results:

**Accuracy**: The model achieved an accuracy of 95.01%, meaning it
correctly predicted arrest or non-arrest cases for the majority of the
observations.

**Sensitivity**: 100%, indicating that the model identified all arrest
cases correctly.

**Specificity**: 0%, indicating that the model failed to correctly
classify any non-arrest cases. This suggests a bias toward predicting
the "no arrest" class due to data imbalance.

**AUC (Area Under the Curve)**: The AUC score was 0.5798, indicating
poor model performance in distinguishing between arrest and non-arrest
cases.

**Significant Features:**

Description was the most significant predictor, indicating that certain
types of crimes or incidents have a higher likelihood of leading to
arrests. Community Area also showed statistical significance, suggesting
that geographic factors influence arrest likelihood. Confusion Matrix:

The model predicted the "**no arrest**" class for nearly all cases,
leading to high accuracy but poor performance in identifying actual
arrest cases.

# Conclusion

Crime Trend and Spatial Analysis in Chicago (2010–2022), provides
valuable insights into the temporal and spatial patterns of crimes in
Chicago over a 13-year period. By combining data cleaning,
visualization, and predictive modeling techniques, we gained a deeper
understanding of how crime evolves over time and varies geographically.

Key findings include:

Crime rates peaked in 2011 and showed a general decline until 2015,
likely reflecting the success of certain crime-reduction measures. The
spike in 2016 suggests either increased crime reporting or specific
events that led to higher crime rates. Stabilization after 2020
indicates consistency in either crime rates or data reporting practices.
Spatial analysis revealed significant geographic variation in crime
distribution, with some areas showing higher concentrations,
underscoring the need for targeted interventions. Predictive modeling
highlighted key factors such as crime type and location, which can guide
resource allocation and preventative strategies.

Overall, this project demonstrates the importance of leveraging
data-driven approaches to understand crime trends and improve public
safety. By identifying patterns and developing predictive tools,
policymakers and law enforcement agencies can better allocate resources,
address crime hotspots, and develop informed strategies for crime
prevention.

# References

1.  Chicago Crime map:
    https://www3.nd.edu/\~skumar5/teaching/additional/spring-2022-eg/project-06-13/index.html
2.  NYC Crime Map:
    https://data.cityofnewyork.us/Public-Safety/Crime-Map-/5jvd-shfj
3.  LA Crime
    Analysis:https://crimegrade.org/violent-crime-los-angeles-ca/
