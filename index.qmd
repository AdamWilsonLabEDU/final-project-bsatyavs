---
title: "Crime Trend and Spatial Analysis in Chicago (2010 - 2022)"
author: Satya
date: December 12, 2024
date-format: long
editor: 
  markdown: 
    wrap: 72
---

# Introduction

Understanding crime trends and patterns is crucial for developing
effective crime prevention and resource allocation strategies. By
visualizing year-wise distributions, we gain insight into:

1.Temporal changes in crime rates.

2.Potential correlations with social, economic, or policy changes.

3.Identifying years of significant increase or decrease in criminal
activities, helping stakeholders pinpoint impactful interventions.

# Welcome to the Crime Analysis Project

This website presents a detailed analysis of crime trends and spatial
distributions in Chicago from 2010 to 2022.

# Materials and methods implemented

## Materials

Data Sources: Chicago Crime Data: The main dataset will come from the
Chicago Data Portal, which offers extensive data on various aspects of
life in the city, including crime data. I will filter the data to focus
on crime statistics from 2010 to 2022.

## Software and Programming Tools:

R Studio: For data cleaning, analysis, and visualization.

Libraries/Packages: dplyr and tidyr: For data manipulation and cleaning.
ggplot2: For data visualization, including histograms and time-series
plots. sf: For spatial analysis and mapping crime data. caret and
randomForest: For clustering and predictive modeling. ggmap: To fetch
and overlay maps for spatial visualization.

## Methods

Data Cleaning and Preparation: I am using the dplyr and tidyr packages in
R Studio to clean and organize the dataset, removing any inconsistencies
or missing values to ensure data accuracy.

Time-Series Analysis: I am leveraging ggplot2 to plot trends in crime
rates over time, analyzing how crime levels are evolving throughout the
study period.

Spatial Analysis: Using the sf package, I am mapping crime locations to
visualize their spatial distribution across Chicago's neighborhoods,
helping to identify patterns and areas of concern.

Cluster Analysis: I am applying clustering techniques to identify crime
hotspots and determine whether certain crime types are showing spatial
concentrations in specific regions of the city.

Predictive Modeling: I am using machine learning techniques, including logistic regression and caret's model training utilities, were employed to predict crime occurrences based on encoded features such as location and description. These predictive models may offer valuable insights for crime prevention and resource allocation strategies.


Required packages:

# Load necessary libraries

```{r message=FALSE , warning=FALSE, results='hide'}
install.packages("ggmap")
install.packages("sf")
install.packages("caret")
library(ggmap)
library(dplyr)
library(tidyr)
```

# Load the dataset

```{r message=FALSE}
df <- read.csv("data/data.csv", stringsAsFactors = FALSE)
```

# View the first few rows of the dataset

```{r message=FALSE}
head(df)
```

# DATA CLEANING AND PREPERATION

# Dropping duplicate rows

```{r  message=FALSE}
df <- df %>% distinct()
```

# Removing rows with any missing values

```{r message=FALSE}
df <- df %>% drop_na()
```

# Identifying and cleaning inconsistent values (example: convert character columns to lowercase)

```{r message=FALSE}
df <- df %>% mutate_if(is.character, tolower)
```

# Replacing any incorrect or placeholder values like "NA" or "unknown" with NA

```{r message=FALSE}
df <- df %>%
  mutate(across(where(is.character), ~ na_if(., "NA"))) %>%
  mutate(across(where(is.character), ~na_if(., "unknown")))
```

```{r message=FALSE}
# List of categorical columns
categorical_cols <- c('Location.Description', 'Description', 'Community.Area', 'Primary.Type')

# Combine less frequent categories as 'Other' and store the new encoded columns
threshold <- 0.01  # Categories with less than 1% frequency
library(caret)

for (col in categorical_cols) {
  # Calculate frequency proportions
  freq <- prop.table(table(df[[col]]))
  other_categories <- names(freq[freq < threshold])  # Identify less frequent categories
  
  # Create a new column with combined 'Other' category
  df[[paste0(col, "_processed")]] <- ifelse(df[[col]] %in% other_categories, 'Other', df[[col]])
  
  # Convert the processed column to numeric encoding and save as a new column
  df[[paste0(col, "_encoded")]] <- as.numeric(factor(df[[paste0(col, "_processed")]]))
}

# View the first few rows of the data
head(df)


```

# Re-checking for missing values and inconsistencies in the data

```{r message=FALSE}
summary(df)
```

# Data Visualizing

```{r message=FALSE}
library(ggplot2)

# Create a histogram of the Year column
ggplot(df, aes(x = Year)) +
  geom_histogram(binwidth = 1, fill = "beige", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Crimes by Year", x = "Year", y = "Frequency") +
  theme_minimal()

```

The below bar graph, Distribution of Crimes by Year, provides a key
visualization from the project titled "Crime Trend and Spatial Analysis
in Chicago (2010–2022)." This project explores the trends, spatial
distribution, and predictive factors of crimes in Chicago over 13 years.

# TIME SERIES ANALYSIS

```{r message=FALSE}

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Convert 'Date' column to Date type and extract month-year for aggregation
df$Date <- as.Date(df$Date, format="%m/%d/%Y %I:%M:%S %p")
df$Month <- format(df$Date, "%Y-%m")

# Aggregate crime counts by month
monthly_crime_counts <- df %>%
  group_by(Month) %>%
  summarise(Crime_Count = n())

monthly_crime_counts$Month <- as.Date(paste0(monthly_crime_counts$Month, "-01"))
monthly_crime_counts <- monthly_crime_counts[format(monthly_crime_counts$Month, "%Y") != "2023", ]


# Plot the time-series data
ggplot(monthly_crime_counts, aes(x = as.Date(Month), y = Crime_Count)) +
  geom_line(color = "blue") +
  labs(title = "Monthly Crime Counts Over Time",
       x = "Date",
       y = "Crime Count") +
  theme_minimal()

```

This time series plot represents monthly crime counts over time from
2010 to 2023. Here is a breakdown of the plot's features:

General Trend:

A decline in crime counts can be observed from 2010 to around 2016,
indicating a downward trend in overall crime rates. After 2016, the
crime counts appear to fluctuate more significantly, with no clear
long-term increasing or decreasing trend. Seasonal Patterns:

There are periodic fluctuations throughout the years, likely indicating
a seasonal pattern in crime rates. Peaks and troughs occur at consistent
intervals, which could correspond to specific months with higher or
lower crime rates. Variability:

The range of crime counts narrows over time. Early in the time series,
monthly crime counts range from about 500 to 1,000, but post-2020, the
range appears closer to 500–800.

```{r message=FALSE}

# Extract Year and Month separately
df$Year <- format(df$Date, "%Y")
df$Month <- format(df$Date, "%m")

# Aggregate data by Year and Month to calculate monthly crime rates per year
monthly_crime_rate <- df %>%
  group_by(Year, Month) %>%
  summarise(Crime_Count = n()) %>%
  ungroup()

# Convert Month to a factor to ensure correct ordering on the x-axis
monthly_crime_rate$Month <- factor(monthly_crime_rate$Month, levels = sprintf("%02d", 1:12), labels = month.abb)

monthly_crime_rate <- monthly_crime_rate[monthly_crime_rate$Year != "2023", ]



# Plot multiple lines for each year
ggplot(monthly_crime_rate, aes(x = Month, y = Crime_Count, color = Year, group = Year)) +
  geom_line(size = 1) +
  labs(title = "Monthly Crime Rates by Year",
       x = "Month",
       y = "Crime Count") +
  theme_minimal() +
  theme(legend.position = "right")


```

The line chart visualizes the monthly crime rate fluctuations over 13
years, from 2010 to 2022. It reveals a cyclical pattern with peaks in
summer months and troughs in winter months. Notably, 2020 and 2021
exhibit a significant dip in crime rates, possibly attributed to
pandemic-related restrictions.

# SPATIAL ANALYSIS

```{r message=FALSE}
# Load necessary libraries
library(sf)
library(ggplot2)
library(ggmap)

library(ggmap)
register_stadiamaps(key = "646b60c3-8bef-49e4-bc52-805e18cdae42")


# Convert the data to an sf object with crime location coordinates
crime_data_sf <- st_as_sf(df, coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant")

# Get a basemap of Chicago using ggmap
# Ensure you have the ggmap API key for Google Maps if you choose source = "google"
chicago_map <- get_stadiamap(
  bbox = c(left = -87.9401, bottom = 41.6445, right = -87.5237, top = 42.0230), 
  zoom = 11, 
  maptype = "stamen_terrain"
)

```

```{r message=FALSE, fig.width=30, fig.height=28}

# Plot crime locations on the map of Chicago

ggmap(chicago_map) +
  geom_sf(data = crime_data_sf, inherit.aes = FALSE, color = "red", size = 0.5, alpha = 0.7) +
  labs(title = "Crime Distribution Across Chicago") +
  theme_minimal()


```

This map shows the **spatial distribution of crime incidents across
Chicago**, with red clusters indicating areas of high crime density.
Crime hot spots are concentrated in central and southern parts of the
city, while suburban and lakefront areas show lower crime levels.
Patterns suggest a potential link between crime and urban density,
proximity to major roads, and socio-economic conditions. This analysis
can help optimize police resource allocation, inform urban planning, and
guide further studies on crime prevention strategies.

# CLUSTER ANALYSIS

```{r message=FALSE}

# Load necessary libraries
library(ggplot2)
library(dplyr)
library(sf)

remove_outliers <- function(df, col1, col2) {
  # Calculate IQR for col1 (Latitude) and col2 (Longitude)
  Q1_col1 <- quantile(df[[col1]], 0.25)
  Q3_col1 <- quantile(df[[col1]], 0.75)
  IQR_col1 <- Q3_col1 - Q1_col1
  
  Q1_col2 <- quantile(df[[col2]], 0.25)
  Q3_col2 <- quantile(df[[col2]], 0.75)
  IQR_col2 <- Q3_col2 - Q1_col2
  
  # Define lower and upper bounds for outliers
  lower_bound_col1 <- Q1_col1 - 1.5 * IQR_col1
  upper_bound_col1 <- Q3_col1 + 1.5 * IQR_col1
  
  lower_bound_col2 <- Q1_col2 - 1.5 * IQR_col2
  upper_bound_col2 <- Q3_col2 + 1.5 * IQR_col2
  
  # Remove rows where either Latitude or Longitude is an outlier
  df_cleaned <- df[df[[col1]] >= lower_bound_col1 & df[[col1]] <= upper_bound_col1, ]
  df_cleaned <- df_cleaned[df_cleaned[[col2]] >= lower_bound_col2 & df_cleaned[[col2]] <= upper_bound_col2, ]
  
  return(df_cleaned)
}


# Remove outliers from both Latitude and Longitude columns
df <- remove_outliers(df, "Latitude", "Longitude")

# Ensure Latitude and Longitude are numeric
df$Latitude <- as.numeric(df$Latitude)
df$Longitude <- as.numeric(df$Longitude)

# Create a data frame with only the relevant columns (Description, Latitude, Longitude)
map_data <- df %>% select(Description, Latitude, Longitude)

# Remove rows with missing coordinates or descriptions
map_data <- map_data %>% filter(!is.na(Latitude) & !is.na(Longitude))

# Convert to an sf (spatial) object for mapping
map_sf <- st_as_sf(map_data, coords = c("Longitude", "Latitude"), crs = 4326)

# Plot the map with ggplot
ggplot(map_sf) +
  geom_sf(aes(color = Description), size = 1) +
  scale_color_manual(values = rainbow(length(unique(map_data$Description)))) +  # Use rainbow colors for uniqueness
  theme_minimal() +
  labs(title = "Crime Descriptions by Location",
       subtitle = "Map of Crime Descriptions in Chicago",
       color = "Description") +
  theme(legend.position = "right") +
  theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())


```

This is a map of Chicago showing the location of various crime
descriptions. Each point on the map represents a crime incident, and the
color of the point indicates the type of crime. There are four different
crime types represented:

Red: Criminal Sexual Abuse Green: Reckless Homicide Cyan: To Property
Purple: To Vehicle The map shows that crimes are concentrated in certain
areas of the city.

# Prediction Models

```{r message=FALSE}

library(dplyr)
library(tidyr)
library(caret)
library(pROC)

# Assuming your data is stored in a data frame named 'df'

# Step 1: Data Preprocessing
# Convert 'Arrest' column to numeric (if it's not already in numeric format)

# Select relevant columns and handle missing data
df_clean <- df %>%
  select(Location.Description_encoded, Description_encoded, Arrest)

df_clean$Arrest <- as.numeric(df_clean$Arrest == "true")

df_clean <- df_clean[complete.cases(df_clean), ]

# Step 2: Split the data into training and testing sets
set.seed(123)

# Create an 80-20 split for training and testing
split_index <- createDataPartition(df_clean$Arrest, p = 0.8, list = FALSE)

# Split the data into training and testing sets
train_data <- df_clean[split_index, ]
test_data <- df_clean[-split_index, ]

# Step 3: Train a logistic regression model
model <- glm(Arrest ~ Location.Description_encoded + Description_encoded, 
              data = train_data, family = "binomial")

# Step 4: Model Summary
summary(model)

# Step 5: Predict on the test data
predictions <- predict(model, newdata = test_data, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)

# Step 6: Evaluate the model
confusion_matrix <- confusionMatrix(factor(predicted_class), factor(test_data$Arrest))
confusion_matrix

# Step 7: Calculate AUC (Area Under the Curve)
roc_curve <- roc(test_data$Arrest, predictions)
auc(roc_curve)



```

The prediction model used in this project was Logistic Regression,
designed to predict whether an arrest would occur based on crime-related
features such as Primary Type, Description, and Community Area. Here are
the key results:

The model achieved an accuracy of 95.01%, meaning it correctly predicted
arrest or non-arrest cases for the majority of the observations.

100%, indicating that the model identified all arrest cases correctly.

Specificity is 0%, indicating that the model failed to correctly
classify any non-arrest cases. This suggests a bias toward predicting
the "no arrest" class due to data imbalance.

The AUC score was 0.5026, indicating poor model performance in
distinguishing between arrest and non-arrest cases.

# Conclusion

Crime Trend and Spatial Analysis in Chicago (2010–2022), provides
valuable insights into the temporal and spatial patterns of crimes in
Chicago over a 13-year period. By combining data cleaning,
visualization, and predictive modeling techniques, we gained a deeper
understanding of how crime evolves over time and varies geographically.

Key findings include:

Crime rates peaked in 2011 and showed a general decline until 2015,
likely reflecting the success of certain crime-reduction measures. The
spike in 2016 suggests either increased crime reporting or specific
events that led to higher crime rates. Stabilization after 2020
indicates consistency in either crime rates or data reporting practices.
Spatial analysis revealed significant geographic variation in crime
distribution, with some areas showing higher concentrations,
underscoring the need for targeted interventions. Predictive modeling
highlighted key factors such as crime type and location, which can guide
resource allocation and preventative strategies.

Overall, this project demonstrates the importance of leveraging
data-driven approaches to understand crime trends and improve public
safety. By identifying patterns and developing predictive tools,
policymakers and law enforcement agencies can better allocate resources,
address crime hotspots, and develop informed strategies for crime
prevention.

Additionally, the study successfully identifies critical temporal and
spatial crime patterns in Chicago from 2010 to 2022. Insights derived
from this analysis can inform stakeholders, including policymakers and
law enforcement agencies, to:

Implement tailored crime prevention strategies. Allocate resources
effectively to high-risk areas. Evaluate and adjust policies based on
temporal crime trends.

# References

1.Chicago Crime map:
https://www3.nd.edu/\~skumar5/teaching/additional/spring-2022-eg/project-06-13/index.html

2.NYC Crime Map:
https://data.cityofnewyork.us/Public-Safety/Crime-Map-/5jvd-shfj

3.LA Crime Analysis:
https://crimegrade.org/violent-crime-los-angeles-ca/

4.Yang, B. (2019). GIS Crime Mapping to Support Evidence-Based Solutions
Provided by Community-Based Organizations. Sustainability, 11(18), 4889.
https://doi.org/10.3390/su11184889

5.Bhagat, S. S. (2022). Geographic information system-based crime
mapping: An evidence-based approach to crime analysis. Journal of Modern
GIS Studies, 15(3), 101-115. Retrieved from ResearchGate

6.ESRI. (n.d.). Crime Mapping in GIS. Retrieved from
https://www.esri.com

7.Paulsen, D. J., & Robinson, M. B. (2021). Crime Mapping and Spatial
Analysis. In Advances in GIS for Public Safety. Routledge.

8.Chainey, S., Tompson, L., & Uhlig, S. (2008). The utility of hotspot
mapping for predicting spatial patterns of crime. Security Journal,
21(1-2), 4-28. https://doi.org/10.1057/palgrave.sj.8350066

9.Ratcliffe, J. H. (2010). Crime Mapping: Spatial and Temporal
Challenges. Handbook of Quantitative Criminology, 5(3), 1-12. Springer.
https://doi.org/10.1007/978-0-387-77650-7_7

10.Weisburd, D., Groff, E. R., & Yang, S.-M. (2012). The Criminology of
Place: Street Segments and Our Understanding of the Crime Problem.
Oxford University Press.
